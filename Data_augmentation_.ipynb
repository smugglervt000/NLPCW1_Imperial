{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H08esTFOYO99"
      },
      "source": [
        "# Main imports and code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EnHQoayhBYlm",
        "outputId": "370560b9-9196-4463-9a89-fda1f42ff013"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Mar  3 17:00:22 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# check which gpu we're using\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYhFR7nSYOjG",
        "outputId": "cd702654-5a1c-49db-b301-cda189465bb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: simpletransformers in /usr/local/lib/python3.10/dist-packages (0.70.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (1.25.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.47.0 in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (4.66.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (2023.12.25)\n",
            "Requirement already satisfied: transformers>=4.31.0 in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (4.38.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (2.18.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (1.2.2)\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (1.2.2)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (2.15.2)\n",
            "Requirement already satisfied: tensorboardx in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (2.6.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (1.5.3)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (0.15.2)\n",
            "Requirement already satisfied: wandb>=0.10.32 in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (0.16.3)\n",
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (1.31.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (0.1.99)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->simpletransformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->simpletransformers) (0.20.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->simpletransformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->simpletransformers) (6.0.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->simpletransformers) (0.4.2)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (8.1.7)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (3.1.42)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (1.40.6)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (0.4.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (3.20.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->simpletransformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->simpletransformers) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->simpletransformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->simpletransformers) (2024.2.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->simpletransformers) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets->simpletransformers) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->simpletransformers) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->simpletransformers) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets->simpletransformers) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets->simpletransformers) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->simpletransformers) (3.9.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->simpletransformers) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->simpletransformers) (2023.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->simpletransformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->simpletransformers) (3.3.0)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit->simpletransformers) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (5.3.3)\n",
            "Requirement already satisfied: importlib-metadata<8,>=1.4 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (7.0.1)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (9.4.0)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (13.7.0)\n",
            "Requirement already satisfied: tenacity<9,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (8.2.3)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (4.10.0)\n",
            "Requirement already satisfied: tzlocal<6,>=1.1 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (5.2)\n",
            "Requirement already satisfied: validators<1,>=0.2 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (0.22.0)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (0.8.1b0)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (6.3.2)\n",
            "Requirement already satisfied: watchdog>=2.1.5 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (4.0.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (1.62.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (3.5.2)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (3.0.1)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit->simpletransformers) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit->simpletransformers) (3.1.3)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit->simpletransformers) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit->simpletransformers) (0.12.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->simpletransformers) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->simpletransformers) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->simpletransformers) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->simpletransformers) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->simpletransformers) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->simpletransformers) (4.0.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb>=0.10.32->simpletransformers) (4.0.11)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->simpletransformers) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->simpletransformers) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard->simpletransformers) (1.3.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<8,>=1.4->streamlit->simpletransformers) (3.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit->simpletransformers) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit->simpletransformers) (2.16.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard->simpletransformers) (2.1.5)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb>=0.10.32->simpletransformers) (5.0.1)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->simpletransformers) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->simpletransformers) (0.33.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->simpletransformers) (0.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit->simpletransformers) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->simpletransformers) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard->simpletransformers) (3.2.2)\n",
            "Requirement already satisfied: tensorboardx in /usr/local/lib/python3.10/dist-packages (2.6.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboardx) (1.25.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardx) (23.2)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardx) (3.20.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install simpletransformers\n",
        "!pip install tensorboardx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJC8wj73Zd_p"
      },
      "outputs": [],
      "source": [
        "from simpletransformers.classification import ClassificationModel, ClassificationArgs, MultiLabelClassificationModel, MultiLabelClassificationArgs\n",
        "from urllib import request\n",
        "import pandas as pd\n",
        "import logging\n",
        "import torch\n",
        "from collections import Counter\n",
        "from ast import literal_eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsX3b7ZNYVZe",
        "outputId": "6edc4f1a-8739-4bae-d2e1-810a10edb251"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cuda available?  True\n"
          ]
        }
      ],
      "source": [
        "# prepare logger\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "transformers_logger = logging.getLogger(\"transformers\")\n",
        "transformers_logger.setLevel(logging.WARNING)\n",
        "\n",
        "# check gpu\n",
        "cuda_available = torch.cuda.is_available()\n",
        "\n",
        "print('Cuda available? ',cuda_available)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpRLLRzkTwdL",
        "outputId": "f971f315-7a24-4117-95d7-cb632fa29f7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ],
      "source": [
        "if cuda_available:\n",
        "  import tensorflow as tf\n",
        "  # Get the GPU device name.\n",
        "  device_name = tf.test.gpu_device_name()\n",
        "  # The device name should look like the following:\n",
        "  if device_name == '/device:GPU:0':\n",
        "      print('Found GPU at: {}'.format(device_name))\n",
        "  else:\n",
        "      raise SystemError('GPU device not found')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMQDATlOZHxu"
      },
      "source": [
        "# Fetch Don't Patronize Me! data manager module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UW903YxwThrH",
        "outputId": "350970ca-d85b-4c27-d82c-3deb23c4be85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching https://raw.githubusercontent.com/Perez-AlmendrosC/dontpatronizeme/master/semeval-2022/dont_patronize_me.py\n"
          ]
        }
      ],
      "source": [
        "module_url = f\"https://raw.githubusercontent.com/Perez-AlmendrosC/dontpatronizeme/master/semeval-2022/dont_patronize_me.py\"\n",
        "module_name = module_url.split('/')[-1]\n",
        "print(f'Fetching {module_url}')\n",
        "#with open(\"file_1.txt\") as f1, open(\"file_2.txt\") as f2\n",
        "with request.urlopen(module_url) as f, open(module_name,'w') as outf:\n",
        "  a = f.read()\n",
        "  outf.write(a.decode('utf-8'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRxm0179aqzw"
      },
      "outputs": [],
      "source": [
        "# helper function to save predictions to an output file\n",
        "def labels2file(p, outf_path):\n",
        "\twith open(outf_path,'w') as outf:\n",
        "\t\tfor pi in p:\n",
        "\t\t\toutf.write(','.join([str(k) for k in pi])+'\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcDThFWVBxGb"
      },
      "outputs": [],
      "source": [
        "from dont_patronize_me import DontPatronizeMe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwLWmQIOeFL9",
        "outputId": "c6b945aa-37a0-46b0-d676-4e84545f9580"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "243SIA6UeQyf"
      },
      "outputs": [],
      "source": [
        "file_path = '/content/drive/MyDrive/NLP CW1/'\n",
        "filename = 'dontpatronizeme_pcl.tsv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJO44ujniNGS"
      },
      "outputs": [],
      "source": [
        "dpm = DontPatronizeMe(file_path, filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2r3USK4eThrJ",
        "outputId": "5a411650-34e8-4a31-d72e-2905c7905111"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Map of label to numerical label:\n",
            "{'Unbalanced_power_relations': 0, 'Shallow_solution': 1, 'Presupposition': 2, 'Authority_voice': 3, 'Metaphors': 4, 'Compassion': 5, 'The_poorer_the_merrier': 6}\n"
          ]
        }
      ],
      "source": [
        "dpm.load_task1()\n",
        "dpm.load_task2(return_one_hot=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0YcdU80IbiS"
      },
      "source": [
        "# Load paragraph IDs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmABsPQfji0p"
      },
      "outputs": [],
      "source": [
        "file1 = '/content/drive/My Drive/NLP CW1/train_semeval_parids-labels.csv'\n",
        "file2 = '/content/drive/My Drive/NLP CW1/dev_semeval_parids-labels.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8AReWYHYOUqx"
      },
      "outputs": [],
      "source": [
        "trids = pd.read_csv(file1)\n",
        "teids = pd.read_csv(file2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7IfCZjwQ16MS"
      },
      "outputs": [],
      "source": [
        "trids.par_id = trids.par_id.astype(str)\n",
        "teids.par_id = teids.par_id.astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFnGtWdHJldw"
      },
      "outputs": [],
      "source": [
        "data=dpm.train_task1_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rmUBJhljbnpN"
      },
      "outputs": [],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lXrNj_Ww_FC"
      },
      "source": [
        "\n",
        "\n",
        "# Rebuild training set (Task 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOxDR1H2g_3p"
      },
      "outputs": [],
      "source": [
        "rows = [] # will contain par_id, label and text\n",
        "for idx in range(len(trids)):\n",
        "  parid = trids.par_id[idx]\n",
        "  #print(parid)\n",
        "  # select row from original dataset to retrieve `text` and binary label\n",
        "  keyword = data.loc[data.par_id == parid].keyword.values[0]\n",
        "  text = data.loc[data.par_id == parid].text.values[0]\n",
        "  label = data.loc[data.par_id == parid].label.values[0]\n",
        "  rows.append({\n",
        "      'par_id':parid,\n",
        "      'community':keyword,\n",
        "      'text':text,\n",
        "      'label':label\n",
        "  })\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7UDCLIgcrCl"
      },
      "outputs": [],
      "source": [
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8e3E08Yown5p"
      },
      "outputs": [],
      "source": [
        "trdf1 = pd.DataFrame(rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pgxtVuhr9ZFG"
      },
      "outputs": [],
      "source": [
        "trdf1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1KGYmpnxDjt"
      },
      "source": [
        "# Rebuild test set (Task 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6FLgB6KxGI2"
      },
      "outputs": [],
      "source": [
        "rows = [] # will contain par_id, label and text\n",
        "for idx in range(len(teids)):\n",
        "  parid = teids.par_id[idx]\n",
        "  #print(parid)\n",
        "  # select row from original dataset\n",
        "  keyword = data.loc[data.par_id == parid].keyword.values[0]\n",
        "  text = data.loc[data.par_id == parid].text.values[0]\n",
        "  label = data.loc[data.par_id == parid].label.values[0]\n",
        "  rows.append({\n",
        "      'par_id':parid,\n",
        "      'community':keyword,\n",
        "      'text':text,\n",
        "      'label':label\n",
        "  })\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbB9GdzJxRAH",
        "outputId": "e25a8a8c-b929-4b88-cc7c-b4de7c6cd1a7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2094"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "len(rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vhBhTRIyxSaQ"
      },
      "outputs": [],
      "source": [
        "tedf1 = pd.DataFrame(rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CuaH7i69aud6"
      },
      "outputs": [],
      "source": [
        "tedf1=tedf1.sample(frac=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1WTDj-_pgrT"
      },
      "outputs": [],
      "source": [
        "tedf1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xK6FY70KZ6TY"
      },
      "source": [
        "# RoBERTa Baseline for Task 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-pvjbu_8h1n"
      },
      "outputs": [],
      "source": [
        "# downsample negative instances\n",
        "pcldf = trdf1[trdf1.label==1]\n",
        "npos = len(pcldf)\n",
        "\n",
        "\n",
        "training_set1 = pd.concat([pcldf,trdf1[trdf1.label==0][:npos*2]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mpSqMp3d8iYu"
      },
      "outputs": [],
      "source": [
        "training_set1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PoW_s23AZ_DG"
      },
      "outputs": [],
      "source": [
        "\n",
        "task1_model_args = ClassificationArgs(num_train_epochs=1,\n",
        "                                      no_save=True,\n",
        "                                      no_cache=True,\n",
        "                                      overwrite_output_dir=True)\n",
        "\n",
        "# Adjust Hyperparameters\n",
        "task1_model_args.num_train_epochs = 3 # Number of epochs\n",
        "task1_model_args.learning_rate = 0.00005 # Learning rate\n",
        "task1_model_args.train_batch_size = 32 # Batch size\n",
        "task1_model_args.weight_decay = 0.01 # Regularization Strength\n",
        "task1_model_args.warmup_steps = 100 # lr starts at 0\n",
        "task1_model_args.gradient_accumulation_steps = 2 # Fancy hyperparameter related to batch size\n",
        "\n",
        "\n",
        "task1_model = ClassificationModel(\"roberta\",\n",
        "                                  'roberta-base',\n",
        "                                  args = task1_model_args,\n",
        "                                  num_labels=2,\n",
        "                                  use_cuda=cuda_available)\n",
        "# train model\n",
        "task1_model.train_model(training_set1[['text', 'label']])\n",
        "# run predictions\n",
        "preds_task1, _ = task1_model.predict(tedf1.text.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5oxHt2R6t2I",
        "outputId": "dfa92109-8ab1-418f-aa86-1fc3e81997db"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({0: 1729, 1: 365})"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "Counter(preds_task1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVWeBU7sYHK-",
        "outputId": "79acf48f-995e-4c85-f7b3-58d8cc5fe366"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 0.8720152817574021\n",
            "precision: 0.4054794520547945\n",
            "recall： 0.7437185929648241\n",
            "f1_score: 0.524822695035461\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
        "print('accuracy: ' + str(accuracy_score(tedf1.label.values, preds_task1)))\n",
        "print('precision: ' + str(precision_score(tedf1.label.values, preds_task1)))\n",
        "print('recall： ' + str(recall_score(tedf1.label.values, preds_task1)))\n",
        "print('f1_score: ' + str(f1_score(tedf1.label.values, preds_task1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzupJuwTafKa"
      },
      "outputs": [],
      "source": [
        "labels2file([[k] for k in preds_task1], 'task1.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7Cc_u5Oli7j"
      },
      "source": [
        "# Rebuild training set (Task 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D2WLYT7wli7k"
      },
      "outputs": [],
      "source": [
        "rows2 = [] # will contain par_id, label and text\n",
        "for idx in range(len(trids)):\n",
        "  parid = trids.par_id[idx]\n",
        "  label = trids.label[idx]\n",
        "  # select row from original dataset to retrieve the `text` value\n",
        "  text = dpm.train_task1_df.loc[dpm.train_task1_df.par_id == parid].text.values[0]\n",
        "  rows2.append({\n",
        "      'par_id':parid,\n",
        "      'text':text,\n",
        "      'label':label\n",
        "  })\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LFqMMb5Jli7l"
      },
      "outputs": [],
      "source": [
        "trdf2 = pd.DataFrame(rows2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HayrC9q7mQPl"
      },
      "outputs": [],
      "source": [
        "trdf2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MxHLB_g0pfEb"
      },
      "outputs": [],
      "source": [
        "trdf2.label = trdf2.label.apply(literal_eval)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gukbmv0bli7l"
      },
      "source": [
        "# Rebuild test set (Task 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjH-AJK1li7m"
      },
      "outputs": [],
      "source": [
        "rows2 = [] # will contain par_id, label and text\n",
        "for idx in range(len(teids)):\n",
        "  parid = teids.par_id[idx]\n",
        "  label = teids.label[idx]\n",
        "  #print(parid)\n",
        "  # select row from original dataset to access the `text` value\n",
        "  text = dpm.train_task1_df.loc[dpm.train_task1_df.par_id == parid].text.values[0]\n",
        "  rows2.append({\n",
        "      'par_id':parid,\n",
        "      'text':text,\n",
        "      'label':label\n",
        "  })\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SRP-tn5wli7n"
      },
      "outputs": [],
      "source": [
        "tedf2 = pd.DataFrame(rows2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8U2lrfJiolku"
      },
      "outputs": [],
      "source": [
        "tedf2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81aQFjWqpbe2"
      },
      "outputs": [],
      "source": [
        "tedf2.label = tedf2.label.apply(literal_eval)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKFiVaslbAiC"
      },
      "source": [
        "# RoBERTa baseline for Task 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmr5ZZf5Ik5T"
      },
      "outputs": [],
      "source": [
        "all_negs = trdf2[trdf2.label.apply(lambda x:sum(x) == 0)]\n",
        "all_pos = trdf2[trdf2.label.apply(lambda x:sum(x) > 0)]\n",
        "\n",
        "training_set2 = pd.concat([all_pos,all_negs[:round(len(all_pos)*0.5)]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zyBcJoHtJHE2"
      },
      "outputs": [],
      "source": [
        "training_set2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECb7_mwzbFa6"
      },
      "outputs": [],
      "source": [
        "task2_model_args = MultiLabelClassificationArgs(num_train_epochs=1,\n",
        "                                                no_save=True,\n",
        "                                                no_cache=True,\n",
        "                                                overwrite_output_dir=True\n",
        "                                                )\n",
        "\n",
        "# Adjust Hyperparameters\n",
        "task2_model_args.num_train_epochs = 3 # Number of epochs\n",
        "task2_model_args.learning_rate = 0.00005 # Learning rate\n",
        "task2_model_args.train_batch_size = 32 # Batch size\n",
        "task2_model_args.weight_decay = 0.01 # Regularization Strength\n",
        "task2_model_args.warmup_steps = 100 # lr starts at 0\n",
        "task2_model_args.gradient_accumulation_steps = 2 # Fancy hyperparameter related to batch size\n",
        "\n",
        "task2_model = MultiLabelClassificationModel(\"roberta\",\n",
        "                                            'roberta-base',\n",
        "                                            num_labels=7,\n",
        "                                            args = task2_model_args,\n",
        "                                            use_cuda=cuda_available)\n",
        "# train model\n",
        "task2_model.train_model(training_set2[['text', 'label']])\n",
        "# run predictions\n",
        "preds_task2, _ = task2_model.predict(tedf2.text.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ly1O4GF0YXON",
        "outputId": "f3a9b41a-6d5c-46c4-9ce0-5b6c97a2bb56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy: 0.6270296084049666\n",
            "precision: 0.8223495702005731\n",
            "recall： 0.9911652340019103\n",
            "f1_score: 0.8756765361349887\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "mlb = MultiLabelBinarizer()\n",
        "\n",
        "y_true = mlb.fit_transform(tedf2.label.values)\n",
        "y_pred = mlb.transform(preds_task2)\n",
        "\n",
        "print('accuracy: ' + str(accuracy_score(y_true, y_pred)))\n",
        "print('precision: ' + str(precision_score(y_true, y_pred, average='samples')))\n",
        "print('recall： ' + str(recall_score(y_true, y_pred, average='samples')))\n",
        "print('f1_score: ' + str(f1_score(y_true, y_pred, average='samples')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0211sxhsbbWZ"
      },
      "outputs": [],
      "source": [
        "labels2file(preds_task2, 'task2.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RT8hjnxbbfJq"
      },
      "source": [
        "## Prepare submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7HICl8MJQf0",
        "outputId": "2bf43c8e-93ef-4443-a203-d5a780d542ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "!cat task1.txt | head -n 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCjziGtxJRif",
        "outputId": "62f9e177-352a-449f-b0af-d8207d1b0892"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1,0,0,0,0,0,0\n",
            "1,0,0,0,0,0,0\n",
            "0,0,0,0,0,0,0\n",
            "1,0,0,0,0,0,0\n",
            "0,0,0,0,0,0,0\n",
            "0,0,0,0,0,0,0\n",
            "1,0,0,0,0,0,0\n",
            "1,0,0,0,0,0,0\n",
            "0,0,0,0,0,0,0\n",
            "1,0,0,0,0,0,0\n"
          ]
        }
      ],
      "source": [
        "!cat task2.txt | head -n 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZDLUcYZbhYg",
        "outputId": "92904f7d-7521-4775-f5e7-4a38a3a2f46d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  adding: task1.txt (deflated 93%)\n",
            "  adding: task2.txt (deflated 97%)\n"
          ]
        }
      ],
      "source": [
        "!zip submission.zip task1.txt task2.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_2VCBilGaH7"
      },
      "source": [
        "## Data Augmentation\n",
        "Reference: https://github.com/makcedward/nlpaug/blob/master/example/textual_augmenter.ipynb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TpYfMdE5cOHU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "data = data.dropna()\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzADm4S3cOJi",
        "outputId": "e15241df-bfa8-44d2-c2fc-5421246d9234"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nlpaug\n",
            "  Downloading nlpaug-1.1.11-py3-none-any.whl (410 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/410.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.2/410.5 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.5/410.5 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (1.25.2)\n",
            "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (2.31.0)\n",
            "Requirement already satisfied: gdown>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (4.7.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (3.13.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (4.66.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (4.12.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2023.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (2024.2.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.5)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (1.7.1)\n",
            "Installing collected packages: nlpaug\n",
            "Successfully installed nlpaug-1.1.11\n"
          ]
        }
      ],
      "source": [
        "!pip install nlpaug\n",
        "\n",
        "\n",
        "import nlpaug.augmenter.char as nac\n",
        "import nlpaug.augmenter.word as naw\n",
        "import nlpaug.augmenter.sentence as nas\n",
        "import nlpaug.flow as nafc\n",
        "\n",
        "from nlpaug.util import Action"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data augmentation and preprocessing"
      ],
      "metadata": {
        "id": "GR9IuBjxrRR8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4oahZ7ZlSJV"
      },
      "source": [
        "## Trying aug = naw.SynonymAug(aug_src='wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ql6sBxYOlP1F"
      },
      "outputs": [],
      "source": [
        "# Initialize the synonym augmenter\n",
        "aug = naw.SynonymAug(aug_src='wordnet')\n",
        "\n",
        "# Function to apply augmentation\n",
        "def augment_text(df):\n",
        "    augmented_texts = []\n",
        "    for text in df['text']:\n",
        "        augmented_text = aug.augment(text)\n",
        "        augmented_texts.append(augmented_text)\n",
        "    return augmented_texts\n",
        "\n",
        "# Apply augmentation to the training set\n",
        "training_set1['augmented_text'] = augment_text(training_set1)\n",
        "\n",
        "data_out = training_set1[['augmented_text', 'label']].rename(columns={'augmented_text': 'text'})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUSoAJ5dlP3c"
      },
      "outputs": [],
      "source": [
        "\n",
        "task1_model_args = ClassificationArgs(num_train_epochs=1,\n",
        "                                      no_save=True,\n",
        "                                      no_cache=True,\n",
        "                                      overwrite_output_dir=True)\n",
        "\n",
        "\n",
        "# Best Hyperparameters\n",
        "task1_model_args.num_train_epochs = 5 # Number of epochs\n",
        "task1_model_args.learning_rate = 3.76e-05 # Learning rate\n",
        "task1_model_args.train_batch_size = 16 # Batch size\n",
        "task1_model_args.weight_decay = 0.0300861 # Regularization Strength\n",
        "task1_model_args.warmup_steps = 222 # lr starts at 0 - goes to 0.0001 over steps\n",
        "task1_model_args.gradient_accumulation_steps = 2 # Fancy hyperparameter related to batch size and memory\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "task1_model_data_aug = ClassificationModel(\"roberta\",\n",
        "                                  'roberta-base',\n",
        "                                  args = task1_model_args,\n",
        "                                  num_labels=2,\n",
        "                                  use_cuda=cuda_available)\n",
        "# train model\n",
        "task1_model_data_aug.train_model(data_out[['text', 'label']])\n",
        "# run predictions\n",
        "preds_task1, _ = task1_model_data_aug.predict(tedf1.text.tolist())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4VcSejSlP56",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93c31c50-879a-419a-fdd6-7577b2de8531"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 0.8767908309455588\n",
            "precision: 0.40390879478827363\n",
            "recall： 0.6231155778894473\n",
            "f1_score: 0.49011857707509887\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
        "print('accuracy: ' + str(accuracy_score(tedf1.label.values, preds_task1)))\n",
        "print('precision: ' + str(precision_score(tedf1.label.values, preds_task1)))\n",
        "print('recall： ' + str(recall_score(tedf1.label.values, preds_task1)))\n",
        "print('f1_score: ' + str(f1_score(tedf1.label.values, preds_task1)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJbGKtx1ld2Z"
      },
      "source": [
        "## Trying aug = naw.AntonymAug()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVihWHdolc0G"
      },
      "outputs": [],
      "source": [
        "aug = naw.AntonymAug()\n",
        "# Function to apply augmentation\n",
        "def augment_text(df):\n",
        "    augmented_texts = []\n",
        "    for text in df['text']:\n",
        "        augmented_text = aug.augment(text)\n",
        "        augmented_texts.append(augmented_text)\n",
        "    return augmented_texts\n",
        "\n",
        "# Apply augmentation to the training set\n",
        "training_set1['augmented_text'] = augment_text(training_set1)\n",
        "\n",
        "data_out = training_set1[['augmented_text', 'label']].rename(columns={'augmented_text': 'text'})\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "task1_model_args = ClassificationArgs(num_train_epochs=1,\n",
        "                                      no_save=True,\n",
        "                                      no_cache=True,\n",
        "                                      overwrite_output_dir=True)\n",
        "\n",
        "\n",
        "# Best Hyperparameters\n",
        "task1_model_args.num_train_epochs = 5 # Number of epochs\n",
        "task1_model_args.learning_rate = 3.76e-05 # Learning rate\n",
        "task1_model_args.train_batch_size = 16 # Batch size\n",
        "task1_model_args.weight_decay = 0.0300861 # Regularization Strength\n",
        "task1_model_args.warmup_steps = 222 # lr starts at 0 - goes to 0.0001 over steps\n",
        "task1_model_args.gradient_accumulation_steps = 2 # Fancy hyperparameter related to batch size and memory\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "task1_model_data_aug = ClassificationModel(\"roberta\",\n",
        "                                  'roberta-base',\n",
        "                                  args = task1_model_args,\n",
        "                                  num_labels=2,\n",
        "                                  use_cuda=cuda_available)\n",
        "# train model\n",
        "task1_model_data_aug.train_model(data_out[['text', 'label']])\n",
        "# run predictions\n",
        "preds_task1, _ = task1_model_data_aug.predict(tedf1.text.tolist())\n"
      ],
      "metadata": {
        "id": "2m6VIeNzrTgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
        "print('accuracy: ' + str(accuracy_score(tedf1.label.values, preds_task1)))\n",
        "print('precision: ' + str(precision_score(tedf1.label.values, preds_task1)))\n",
        "print('recall： ' + str(recall_score(tedf1.label.values, preds_task1)))\n",
        "print('f1_score: ' + str(f1_score(tedf1.label.values, preds_task1)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbJSP8--rVfU",
        "outputId": "3c34010d-246f-4d45-b43c-6fe56fcfce44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 0.8500477554918816\n",
            "precision: 0.3566084788029925\n",
            "recall： 0.7185929648241206\n",
            "f1_score: 0.4766666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Roberta base"
      ],
      "metadata": {
        "id": "g_0QLPKPF-7J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yeWYTx0Bly94"
      },
      "outputs": [],
      "source": [
        "# Run Bert Base case\n",
        "torch.manual_seed(2)\n",
        "\n",
        "\n",
        "# Best Hyperparameters\n",
        "task1_model_args.num_train_epochs = 5 # Number of epochs\n",
        "task1_model_args.learning_rate = 3.76e-05 # Learning rate\n",
        "task1_model_args.train_batch_size = 16 # Batch size\n",
        "task1_model_args.weight_decay = 0.0300861 # Regularization Strength\n",
        "task1_model_args.warmup_steps = 222 # lr starts at 0 - goes to 0.0001 over steps\n",
        "task1_model_args.gradient_accumulation_steps = 2 # Fancy hyperparameter related to batch size and memory\n",
        "\n",
        "\n",
        "\n",
        "task1_model_data_aug = ClassificationModel(\"roberta\",\n",
        "                                  'roberta-base',\n",
        "                                  args = task1_model_args,\n",
        "                                  num_labels=2,\n",
        "                                  use_cuda=cuda_available)\n",
        "# train model\n",
        "task1_model_data_aug.train_model(training_set1[['text', 'label']])\n",
        "# run predictions\n",
        "preds_task1_bert, _ = task1_model.predict(tedf1.text.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXK6895nlzAS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f304aec-3b7e-4522-a35e-448d401b95ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 0.8519579751671442\n",
            "precision: 0.36561743341404357\n",
            "recall： 0.7587939698492462\n",
            "f1_score: 0.49346405228758167\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
        "print('accuracy: ' + str(accuracy_score(tedf1.label.values, preds_task1)))\n",
        "print('precision: ' + str(precision_score(tedf1.label.values, preds_task1)))\n",
        "print('recall： ' + str(recall_score(tedf1.label.values, preds_task1)))\n",
        "print('f1_score: ' + str(f1_score(tedf1.label.values, preds_task1)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bert model"
      ],
      "metadata": {
        "id": "b1mTeHOw597h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PbJKGaqwlzC8"
      },
      "outputs": [],
      "source": [
        "# Run Bert Base case\n",
        "torch.manual_seed(2)\n",
        "\n",
        "\n",
        "# Best Hyperparameters\n",
        "task1_model_args.num_train_epochs = 5 # Number of epochs\n",
        "task1_model_args.learning_rate = 3.76e-05 # Learning rate\n",
        "task1_model_args.train_batch_size = 16 # Batch size\n",
        "task1_model_args.weight_decay = 0.0300861 # Regularization Strength\n",
        "task1_model_args.warmup_steps = 222 # lr starts at 0 - goes to 0.0001 over steps\n",
        "task1_model_args.gradient_accumulation_steps = 2 # Fancy hyperparameter related to batch size and memory\n",
        "\n",
        "\n",
        "task1_model_bert_aug = ClassificationModel(\"bert\",\n",
        "                                    'bert-base-uncased',\n",
        "                                  args = task1_model_args,\n",
        "                                  num_labels=2,\n",
        "                                  use_cuda=cuda_available)\n",
        "# train model\n",
        "task1_model_bert_aug.train_model(training_set1[['text', 'label']])\n",
        "# run predictions\n",
        "preds_task1_bert, _ = task1_model.predict(tedf1.text.tolist())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate Bert model\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
        "print('precision: ' + str(precision_score(tedf1.label.values, preds_task1_bert)))\n",
        "print( 'recall： ' + str(recall_score(tedf1.label.values, preds_task1_bert)))\n",
        "print('f1_score: ' + str(f1_score(tedf1.label.values, preds_task1_bert)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvJ2MofkFI3N",
        "outputId": "44b9b981-dbf4-4699-b406-e7158f23aa47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "precision: 0.36561743341404357\n",
            "recall： 0.7587939698492462\n",
            "f1_score: 0.49346405228758167\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XLnet"
      ],
      "metadata": {
        "id": "DzKjRZUF6Xm-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kx7QZlkLkYtC"
      },
      "outputs": [],
      "source": [
        "# Run XLNet Base case\n",
        "torch.manual_seed(2)\n",
        "\n",
        "\n",
        "# Best Hyperparameters\n",
        "task1_model_args.num_train_epochs = 5 # Number of epochs\n",
        "task1_model_args.learning_rate = 3.76e-05 # Learning rate\n",
        "task1_model_args.train_batch_size = 16 # Batch size\n",
        "task1_model_args.weight_decay = 0.0300861 # Regularization Strength\n",
        "task1_model_args.warmup_steps = 222 # lr starts at 0 - goes to 0.0001 over steps\n",
        "task1_model_args.gradient_accumulation_steps = 2 # Fancy hyperparameter related to batch size and memory\n",
        "\n",
        "\n",
        "task1_model_xlnet = ClassificationModel(\"xlnet\",\n",
        "                                    'xlnet-base-cased',\n",
        "                                    args = task1_model_args,\n",
        "                                    num_labels=2,\n",
        "                                    use_cuda=cuda_available)\n",
        "\n",
        "# train model\n",
        "task1_model_xlnet.train_model(training_set1[['text', 'label']])\n",
        "# run predictions\n",
        "preds_task1_xlnet, _ = task1_model_xlnet.predict(tedf1.text.tolist())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate XLNet model\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
        "print('precision: ' + str(precision_score(tedf1.label.values, preds_task1_xlnet)))\n",
        "print( 'recall： ' + str(recall_score(tedf1.label.values, preds_task1_xlnet)))\n",
        "print('f1_score: ' + str(f1_score(tedf1.label.values, preds_task1_xlnet)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHNEJHy_EVHR",
        "outputId": "ecf98be3-32d7-4cba-a087-6ead5fc226f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "precision: 0.421875\n",
            "recall： 0.678391959798995\n",
            "f1_score: 0.5202312138728324\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DistilBERT model"
      ],
      "metadata": {
        "id": "2b93avM26zAD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run DistilBert Base case\n",
        "torch.manual_seed(2)\n",
        "# Best Hyperparameters\n",
        "task1_model_args.num_train_epochs = 5 # Number of epochs\n",
        "task1_model_args.learning_rate = 3.76e-05 # Learning rate\n",
        "task1_model_args.train_batch_size = 16 # Batch size\n",
        "task1_model_args.weight_decay = 0.0300861 # Regularization Strength\n",
        "task1_model_args.warmup_steps = 222 # lr starts at 0 - goes to 0.0001 over steps\n",
        "task1_model_args.gradient_accumulation_steps = 2 # Fancy hyperparameter related to batch size and memory\n",
        "\n",
        "task1_model_distilbert = ClassificationModel(\"distilbert\",\n",
        "                                  'distilbert-base-uncased',\n",
        "                                  args = task1_model_args,\n",
        "                                  num_labels=2,\n",
        "                                  use_cuda=cuda_available)\n",
        "\n",
        "# train model\n",
        "task1_model_distilbert.train_model(training_set1[['text', 'label']])\n",
        "# run predictions\n",
        "preds_task1_distilbert, _ = task1_model_distilbert.predict(tedf1.text.tolist())"
      ],
      "metadata": {
        "id": "LuXGKvUMDJli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
        "print('precision: ' + str(precision_score(tedf1.label.values, preds_task1_distilbert)))\n",
        "print( 'recall： ' + str(recall_score(tedf1.label.values, preds_task1_distilbert)))\n",
        "print('f1_score: ' + str(f1_score(tedf1.label.values, preds_task1_distilbert)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wAEuTiHDdBr",
        "outputId": "d44779a0-e377-470b-cfd1-c867a391c288"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "precision: 0.40978593272171254\n",
            "recall： 0.6733668341708543\n",
            "f1_score: 0.5095057034220531\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data preprocessing\n",
        "## backtranslation"
      ],
      "metadata": {
        "id": "b8-_YBAd7Bjc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install googletrans\n",
        "!pip install -U deep-translator\n",
        "from deep_translator import GoogleTranslator\n",
        "GoogleTranslator(source='auto', target='de').translate(\"it is a sunny day today\")"
      ],
      "metadata": {
        "id": "SiV3MTRM66es"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from googletrans import Translator\n",
        "from deep_translator import GoogleTranslator\n",
        "\n",
        "# Load the data\n",
        "df = trdf1 #pd.read_csv(\"data.csv\")\n",
        "\n",
        "# Get the examples with label = 1\n",
        "label_1 = df[df[\"label\"] == 1]\n",
        "\n",
        "# Initialize the Translator\n",
        "translator = Translator()\n",
        "\n",
        "# Create a list to store the translated text\n",
        "translated_text = []\n",
        "\n",
        "# Loop over the examples with label = 1\n",
        "for index, row in label_1.iterrows():\n",
        "    # Translate the text from English to French\n",
        "    # translated = translator.translate(row[\"text\"], dest=\"fr\").text\n",
        "    translated=GoogleTranslator(source='en', target='fr').translate(row[\"text\"])\n",
        "\n",
        "    # Translate the text back from French to English\n",
        "    # back_translated = translator.translate(translated, dest=\"en\").text\n",
        "    back_translated=GoogleTranslator(source='fr', target='en').translate(translated)\n",
        "    # Add the back-translated text to the list\n",
        "    translated_text.append(back_translated)\n",
        "\n",
        "# Add the back-translated text to the original dataframe\n",
        "label_1[\"back_translated_text\"] = translated_text\n",
        "\n",
        "# Concatenate the original dataframe and the back-translated dataframe\n",
        "df = pd.concat([df, label_1])\n",
        "\n",
        "Label_1_cleaned = label_1.drop(['text'], axis=1)\n",
        "\n",
        "#rename the column back_translated_text to text\n",
        "Label_1_cleaned.rename(columns = {'back_translated_text':'text'}, inplace = True)\n",
        "# Concatenate Label_1_cleaned below the training_set1\n",
        "training_set1_backtranslate_french = pd.concat([trdf1, Label_1_cleaned])\n",
        "training_set1_backtranslate_french\n"
      ],
      "metadata": {
        "id": "hFdJOUhF66hl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the minority class from training_set1\n",
        "training_set1_backtranslate_french[training_set1_backtranslate_french[\"label\"] == 1]\n",
        "# Print the number of examples in the minority class and the majority class\n",
        "print(\"No. of examples in minority class:\", len(training_set1_backtranslate_french[training_set1_backtranslate_french[\"label\"] == 1]))\n",
        "print(\"No. of examples in majority class:\", len(training_set1_backtranslate_french[training_set1_backtranslate_french[\"label\"] == 0]))\n",
        "duplicates= training_set1_backtranslate_french[training_set1_backtranslate_french[\"label\"] == 1].text.duplicated().sum()\n",
        "print(\"Number of duplicates in the minority class:\", duplicates)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUvxA6an66k2",
        "outputId": "37f1b376-9fd3-4be2-e97e-d68af3c39088"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No. of examples in minority class: 1588\n",
            "No. of examples in majority class: 7581\n",
            "Number of duplicates in the minority class: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# downsample negative instances\n",
        "pcldf_french = training_set1_backtranslate_french[training_set1_backtranslate_french.label==1]\n",
        "npos = len(pcldf_french)\n",
        "print(npos)\n",
        "\n",
        "training_set1_french = pd.concat([pcldf_french,training_set1_backtranslate_french[training_set1_backtranslate_french.label==0][:npos*2]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KjhOQiPq6HcF",
        "outputId": "f56bfbc8-46e1-47a4-d235-392932045e5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1588\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print the number of positive and negative instances in the training set\n",
        "print(training_set1_french.label.value_counts())\n",
        "\n",
        "# Print by how much the negative instances are downsampled in absolute terms\n",
        "print(f'Number of negative instances downsampled: {len(training_set1_backtranslate_french[training_set1_backtranslate_french.label==0]) - len(training_set1_backtranslate_french[training_set1_backtranslate_french.label==0])}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyOhUERP81vu",
        "outputId": "5e74653d-022c-4a19-a6bd-fcf840d70917"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    3176\n",
            "1    1588\n",
            "Name: label, dtype: int64\n",
            "Number of negative instances downsampled: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run Roberta Base case\n",
        "\n",
        "torch.manual_seed(2)\n",
        "\n",
        "# Best Hyperparameters\n",
        "task1_model_args.num_train_epochs = 5 # Number of epochs\n",
        "task1_model_args.learning_rate = 3.76e-05 # Learning rate\n",
        "task1_model_args.train_batch_size = 16 # Batch size\n",
        "task1_model_args.weight_decay = 0.0300861 # Regularization Strength\n",
        "task1_model_args.warmup_steps = 222 # lr starts at 0 - goes to 0.0001 over steps\n",
        "task1_model_args.gradient_accumulation_steps = 2 # Fancy hyperparameter related to batch size and memory\n",
        "\n",
        "task1_model = ClassificationModel(\"roberta\",\n",
        "                                  'roberta-base',\n",
        "                                  args = task1_model_args,\n",
        "                                  num_labels=2,\n",
        "                                  use_cuda=cuda_available)\n",
        "# train model\n",
        "task1_model.train_model(training_set1_french[['text', 'label']])\n",
        "# run predictions\n",
        "preds_task1, _ = task1_model.predict(tedf1.text.tolist())"
      ],
      "metadata": {
        "id": "IjmHZvoo82bS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
        "print('precision: ' + str(precision_score(tedf1.label.values, preds_task1)))\n",
        "print( 'recall： ' + str(recall_score(tedf1.label.values, preds_task1)))\n",
        "print('f1_score: ' + str(f1_score(tedf1.label.values, preds_task1)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyqoCqGsHbWJ",
        "outputId": "5e99ed95-86fd-4a5f-fa3a-077315383fda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "precision: 0.4981949458483754\n",
            "recall： 0.6934673366834171\n",
            "f1_score: 0.5798319327731092\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing a learning rate scheduler"
      ],
      "metadata": {
        "id": "l7u3AWnHNnoh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
        "\n",
        "enable_lr_scheduling = True\n",
        "\n",
        "# Set up early stopping and other parameters\n",
        "task1_model_args = ClassificationArgs()\n",
        "task1_model_args.num_train_epochs = 5\n",
        "task1_model_args.learning_rate = 3.76e-05\n",
        "task1_model_args.train_batch_size = 16\n",
        "task1_model_args.weight_decay = 0.0300861\n",
        "task1_model_args.warmup_steps = 222\n",
        "task1_model_args.gradient_accumulation_steps = 2\n",
        "task1_model_args.use_early_stopping = True\n",
        "task1_model_args.early_stopping_patience = 3\n",
        "task1_model_args.early_stopping_delta = 0.01\n",
        "task1_model_args.early_stopping_metric = \"f1\"\n",
        "task1_model_args.early_stopping_metric_minimize = False\n",
        "task1_model_args.save_steps = -1  # Prevents saving checkpoints\n",
        "task1_model_args.save_model_every_epoch = False\n",
        "task1_model_args.no_save = True  # Disable saving model after training\n",
        "task1_model_args.overwrite_output_dir = True\n",
        "\n",
        "# Conditionally set the learning rate scheduler\n",
        "if enable_lr_scheduling:\n",
        "    task1_model_args.scheduler = \"linear_schedule_with_warmup\"\n",
        "else:\n",
        "    task1_model_args.scheduler = \"constant_schedule\"\n",
        "\n",
        "# Train the model with the conditional learning rate scheduler\n",
        "model = ClassificationModel(\n",
        "    \"roberta\",\n",
        "    'roberta-base',\n",
        "    args=task1_model_args,\n",
        "    num_labels=2,\n",
        "    use_cuda=cuda_available\n",
        ")\n",
        "\n",
        "# Train and evaluate the model\n",
        "model.train_model(training_set1_french[['text', 'label']], eval_df=tedf1[['text', 'label']])\n",
        "results, model_outputs, wrong_predictions = model.eval_model(tedf1[['text', 'label']])\n"
      ],
      "metadata": {
        "id": "Tprw9_C_NquD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Arguments for training without a learning rate scheduler\n",
        "task1_model_args_no_scheduler = ClassificationArgs()\n",
        "task1_model_args_no_scheduler.num_train_epochs = 5\n",
        "task1_model_args_no_scheduler.learning_rate = 3.76e-05\n",
        "task1_model_args_no_scheduler.train_batch_size = 16\n",
        "task1_model_args_no_scheduler.weight_decay = 0.0300861\n",
        "task1_model_args_no_scheduler.use_early_stopping = True\n",
        "task1_model_args_no_scheduler.early_stopping_patience = 3\n",
        "task1_model_args_no_scheduler.early_stopping_delta = 0.01\n",
        "task1_model_args_no_scheduler.early_stopping_metric = \"f1\"\n",
        "task1_model_args_no_scheduler.early_stopping_metric_minimize = False\n",
        "task1_model_args_no_scheduler.save_steps = -1\n",
        "task1_model_args_no_scheduler.save_model_every_epoch = False\n",
        "task1_model_args_no_scheduler.no_save = True\n",
        "task1_model_args_no_scheduler.overwrite_output_dir = True\n",
        "\n",
        "# Train the model without a learning rate scheduler and collect the results\n",
        "model_without_scheduler = ClassificationModel(\n",
        "    \"roberta\",\n",
        "    'roberta-base',\n",
        "    args=task1_model_args_no_scheduler,  # Arguments without scheduler settings\n",
        "    num_labels=2,\n",
        "    use_cuda=cuda_available\n",
        ")\n",
        "# Train and evaluate the model\n",
        "model_without_scheduler.train_model(training_set1_french[['text', 'label']], eval_df=tedf1[['text', 'label']])\n",
        "result_without_scheduler, model_outputs, wrong_predictions = model_without_scheduler.eval_model(tedf1[['text', 'label']])\n",
        "\n"
      ],
      "metadata": {
        "id": "hNBOLCzQNsis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "loss_with_scheduler = results['eval_loss']\n",
        "f1_with_scheduler = results['f1_score']\n",
        "loss_without_scheduler = result_without_scheduler['eval_loss']\n",
        "f1_without_scheduler = result_without_scheduler['f1_score']\n",
        "\n",
        "metrics = ['Loss', 'F1 Score']\n",
        "values_with_scheduler = [loss_with_scheduler, f1_with_scheduler]\n",
        "values_without_scheduler = [loss_without_scheduler, f1_without_scheduler]\n",
        "\n",
        "x = np.arange(len(metrics))  # the label locations\n",
        "width = 0.35  # the width of the bars\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "rects1 = ax.bar(x - width/2, values_with_scheduler, width, label='With Scheduler')\n",
        "rects2 = ax.bar(x + width/2, values_without_scheduler, width, label='Without Scheduler')\n",
        "\n",
        "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
        "ax.set_ylabel('Scores')\n",
        "ax.set_title('Scores by metric and scheduler use')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(metrics)\n",
        "ax.legend()\n",
        "\n",
        "ax.bar_label(rects1, padding=3)\n",
        "ax.bar_label(rects2, padding=3)\n",
        "\n",
        "fig.tight_layout()\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "omo4ZPpQaZFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmFS8lcMbTeS",
        "outputId": "8c66d4f5-ef0b-45e7-ddf6-5c2e31491a1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.16.3)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.42)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.40.6)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login()\n"
      ],
      "metadata": {
        "id": "q3tiG4dVbubk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
        "import wandb\n",
        "\n",
        "wandb.login()\n",
        "\n",
        "enable_lr_scheduling = True\n",
        "\n",
        "# Set up all parameters including WandB project name in a single ClassificationArgs configuration\n",
        "task1_model_args = ClassificationArgs(\n",
        "    num_train_epochs=5,\n",
        "    learning_rate=3.76e-05,\n",
        "    train_batch_size=16,\n",
        "    weight_decay=0.0300861,\n",
        "    warmup_steps=222,\n",
        "    gradient_accumulation_steps=2,\n",
        "    use_early_stopping=True,\n",
        "    early_stopping_patience=3,\n",
        "    early_stopping_delta=0.01,\n",
        "    early_stopping_metric=\"f1\",\n",
        "    early_stopping_metric_minimize=False,\n",
        "    save_steps=-1,  # Prevents saving checkpoints\n",
        "    save_model_every_epoch=False,\n",
        "    no_save=True,  # Disable saving model after training\n",
        "    overwrite_output_dir=True,\n",
        "    wandb_project=\"learningratescheduler\",\n",
        ")\n",
        "\n",
        "# Conditional setting for the learning rate scheduler\n",
        "task1_model_args.scheduler = \"linear_schedule_with_warmup\" if enable_lr_scheduling else \"constant_schedule\"\n",
        "\n",
        "# Initialize and train the model with the configured settings\n",
        "model = ClassificationModel(\n",
        "    \"roberta\",\n",
        "    'roberta-base',\n",
        "    args=task1_model_args,\n",
        "    num_labels=2,\n",
        "    use_cuda=torch.cuda.is_available()  # Automatically use CUDA if available\n",
        ")\n",
        "\n",
        "model.train_model(training_set1_french[['text', 'label']])\n",
        "results, model_outputs, wrong_predictions = model.eval_model(tedf1[['text', 'label']])\n"
      ],
      "metadata": {
        "id": "U7To_v20fWKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
        "import wandb\n",
        "\n",
        "# Ensure you're logged into WandB; this might prompt for an API key if you're not already logged in.\n",
        "wandb.login()\n",
        "\n",
        "enable_lr_scheduling = False\n",
        "\n",
        "# Set up all parameters including WandB project name in a single ClassificationArgs configuration\n",
        "task1_model_args_no_scheduler = ClassificationArgs(\n",
        "    num_train_epochs=5,\n",
        "    learning_rate=3.76e-05,\n",
        "    train_batch_size=16,\n",
        "    weight_decay=0.0300861,\n",
        "    use_early_stopping=True,\n",
        "    early_stopping_patience=3,\n",
        "    early_stopping_delta=0.01,\n",
        "    early_stopping_metric=\"f1\",\n",
        "    early_stopping_metric_minimize=False,\n",
        "    save_steps=-1,  # Prevents saving checkpoints\n",
        "    save_model_every_epoch=False,\n",
        "    no_save=True,  # Disable saving model after training\n",
        "    overwrite_output_dir=True,\n",
        "    wandb_project=\"learningratescheduler_no_scheduler\",  # A different WandB project name for clarity\n",
        ")\n",
        "\n",
        "\n",
        "# Initialize and train the model without a learning rate scheduler\n",
        "model_without_scheduler = ClassificationModel(\n",
        "    \"roberta\",\n",
        "    'roberta-base',\n",
        "    args=task1_model_args_no_scheduler,\n",
        "    num_labels=2,\n",
        "    use_cuda=torch.cuda.is_available()  # Automatically use CUDA if available\n",
        ")\n",
        "\n",
        "model_without_scheduler.train_model(training_set1_french[['text', 'label']])\n",
        "results_without_scheduler, model_outputs, wrong_predictions = model_without_scheduler.eval_model(tedf1[['text', 'label']])\n"
      ],
      "metadata": {
        "id": "Mxo-tQ-Bf2L3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random upsampling"
      ],
      "metadata": {
        "id": "3Xi1fNCyNluZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import resample\n",
        "\n",
        "# Separate majority and minority classes\n",
        "df_majority = training_set1[training_set1.label==0]\n",
        "df_minority = training_set1[training_set1.label==1]\n",
        "\n",
        "# Upsample minority class\n",
        "df_minority_upsampled = resample(df_minority,\n",
        "                                 replace=True,     # sample with replacement\n",
        "                                 n_samples=len(df_majority),    # to match majority class\n",
        "                                 random_state=123) # reproducible results\n",
        "\n",
        "# Combine majority class with upsampled minority class\n",
        "df_upsampled = pd.concat([df_majority, df_minority_upsampled])\n",
        "\n",
        "# Display new class counts\n",
        "df_upsampled.label.value_counts()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBJfb4BE8_aF",
        "outputId": "318801b3-cbb8-412d-c7fe-f82bd9f0a405"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    1588\n",
              "1    1588\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run Roberta Base case\n",
        "\n",
        "torch.manual_seed(2)\n",
        "\n",
        "# Best Hyperparameters\n",
        "task1_model_args.num_train_epochs = 5 # Number of epochs\n",
        "task1_model_args.learning_rate = 3.76e-05 # Learning rate\n",
        "task1_model_args.train_batch_size = 16 # Batch size\n",
        "task1_model_args.weight_decay = 0.0300861 # Regularization Strength\n",
        "task1_model_args.warmup_steps = 222 # lr starts at 0 - goes to 0.0001 over steps\n",
        "task1_model_args.gradient_accumulation_steps = 2 # Fancy hyperparameter related to batch size and memory\n",
        "\n",
        "task1_model = ClassificationModel(\"roberta\",\n",
        "                                  'roberta-base',\n",
        "                                  args = task1_model_args,\n",
        "                                  num_labels=2,\n",
        "                                  use_cuda=cuda_available)\n",
        "# train model\n",
        "task1_model.train_model(df_upsampled[['text', 'label']])\n",
        "# run predictions\n",
        "preds_task1, _ = task1_model.predict(tedf1.text.tolist())"
      ],
      "metadata": {
        "id": "2X5WunPd8_qD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
        "print('precision: ' + str(precision_score(tedf1.label.values, preds_task1)))\n",
        "print( 'recall： ' + str(recall_score(tedf1.label.values, preds_task1)))\n",
        "print('f1_score: ' + str(f1_score(tedf1.label.values, preds_task1)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGoqkxWcMIYD",
        "outputId": "0648585d-290a-4507-801f-1408804cff7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "precision: 0.4794007490636704\n",
            "recall： 0.6432160804020101\n",
            "f1_score: 0.5493562231759657\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run Roberta Base case\n",
        "\n",
        "torch.manual_seed(2)\n",
        "\n",
        "# Best Hyperparameters\n",
        "task1_model_args.num_train_epochs = 5 # Number of epochs\n",
        "task1_model_args.learning_rate = 3.76e-05 # Learning rate\n",
        "task1_model_args.train_batch_size = 16 # Batch size\n",
        "task1_model_args.weight_decay = 0.0300861 # Regularization Strength\n",
        "task1_model_args.warmup_steps = 222 # lr starts at 0 - goes to 0.0001 over steps\n",
        "task1_model_args.gradient_accumulation_steps = 2 # Fancy hyperparameter related to batch size and memory\n",
        "\n",
        "task1_model_distilbert = ClassificationModel(\"distilbert\",\n",
        "                                  'distilbert-base-uncased',\n",
        "                                  args = task1_model_args,\n",
        "                                  num_labels=2,\n",
        "                                  use_cuda=cuda_available)\n",
        "# train model\n",
        "task1_model_distilbert.train_model(df_upsampled[['text', 'label']])\n",
        "# run predictions\n",
        "preds_task1, _ = task1_model_distilbert.predict(tedf1.text.tolist())"
      ],
      "metadata": {
        "id": "cBDKhblnvWlZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
        "print('precision: ' + str(precision_score(tedf1.label.values, preds_task1)))\n",
        "print( 'recall： ' + str(recall_score(tedf1.label.values, preds_task1)))\n",
        "print('f1_score: ' + str(f1_score(tedf1.label.values, preds_task1)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YQoY8AKvrlF",
        "outputId": "76568632-4327-4a7f-b929-28db7b03e4c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "precision: 0.4406779661016949\n",
            "recall： 0.6532663316582915\n",
            "f1_score: 0.5263157894736842\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "torch.manual_seed(2)\n",
        "\n",
        "# Best Hyperparameters\n",
        "task1_model_args.num_train_epochs = 5 # Number of epochs\n",
        "task1_model_args.learning_rate = 3.76e-05 # Learning rate\n",
        "task1_model_args.train_batch_size = 16 # Batch size\n",
        "task1_model_args.weight_decay = 0.0300861 # Regularization Strength\n",
        "task1_model_args.warmup_steps = 222 # lr starts at 0 - goes to 0.0001 over steps\n",
        "task1_model_args.gradient_accumulation_steps = 2 # Fancy hyperparameter related to batch size and memory\n",
        "\n",
        "task1_model_xlnet = ClassificationModel(\"xlnet\",\n",
        "                                    'xlnet-base-cased',\n",
        "                                    args = task1_model_args,\n",
        "                                    num_labels=2,\n",
        "                                    use_cuda=cuda_available)\n",
        "\n",
        "# train model\n",
        "task1_model_xlnet.train_model(df_upsampled[['text', 'label']])\n",
        "# run predictions\n",
        "preds_task1, _ = task1_model_xlnet.predict(tedf1.text.tolist())"
      ],
      "metadata": {
        "id": "VJknht_fvxIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
        "print('precision: ' + str(precision_score(tedf1.label.values, preds_task1)))\n",
        "print( 'recall： ' + str(recall_score(tedf1.label.values, preds_task1)))\n",
        "print('f1_score: ' + str(f1_score(tedf1.label.values, preds_task1)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4sGHsl5wCaA",
        "outputId": "a5632f01-ea08-41ff-e9b9-054fc3103ac9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "precision: 0.4099722991689751\n",
            "recall： 0.7437185929648241\n",
            "f1_score: 0.5285714285714287\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(2)\n",
        "\n",
        "# Best Hyperparameters\n",
        "task1_model_args.num_train_epochs = 5 # Number of epochs\n",
        "task1_model_args.learning_rate = 3.76e-05 # Learning rate\n",
        "task1_model_args.train_batch_size = 16 # Batch size\n",
        "task1_model_args.weight_decay = 0.0300861 # Regularization Strength\n",
        "task1_model_args.warmup_steps = 222 # lr starts at 0 - goes to 0.0001 over steps\n",
        "task1_model_args.gradient_accumulation_steps = 2 # Fancy hyperparameter related to batch size and memory\n",
        "\n",
        "task1_model_bert_aug = ClassificationModel(\"bert\",\n",
        "                                    'bert-base-uncased',\n",
        "                                  args = task1_model_args,\n",
        "                                  num_labels=2,\n",
        "                                  use_cuda=cuda_available)\n",
        "# train model\n",
        "task1_model_bert_aug.train_model(training_set1[['text', 'label']])\n",
        "# run predictions\n",
        "preds_task1_bert, _ = task1_model_bert_aug.predict(tedf1.text.tolist())\n"
      ],
      "metadata": {
        "id": "DlSap5E_wJK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
        "print('precision: ' + str(precision_score(tedf1.label.values, preds_task1)))\n",
        "print( 'recall： ' + str(recall_score(tedf1.label.values, preds_task1)))\n",
        "print('f1_score: ' + str(f1_score(tedf1.label.values, preds_task1)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umHpDmxBwSna",
        "outputId": "8cd7222c-b2cd-45cf-8eba-7ebc047e6d70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "precision: 0.4099722991689751\n",
            "recall： 0.7437185929648241\n",
            "f1_score: 0.5285714285714287\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## random upsampling and downsampling"
      ],
      "metadata": {
        "id": "Nt1M_t4DOq5J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Downsample majority class\n",
        "df_majority_downsampled = resample(df_majority,\n",
        "                                   replace=False,    # sample without replacement\n",
        "                                   n_samples=len(df_minority_upsampled), # match minority class\n",
        "                                   random_state=123) # reproducible results\n",
        "\n",
        "# Combine downsampled majority class with upsampled minority class\n",
        "df_balanced = pd.concat([df_minority_upsampled, df_majority_downsampled])\n",
        "\n"
      ],
      "metadata": {
        "id": "AItvCIQqN5aU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run Roberta Base case\n",
        "\n",
        "torch.manual_seed(2)\n",
        "\n",
        "# Best Hyperparameters\n",
        "task1_model_args.num_train_epochs = 5 # Number of epochs\n",
        "task1_model_args.learning_rate = 3.76e-05 # Learning rate\n",
        "task1_model_args.train_batch_size = 16 # Batch size\n",
        "task1_model_args.weight_decay = 0.0300861 # Regularization Strength\n",
        "task1_model_args.warmup_steps = 222 # lr starts at 0 - goes to 0.0001 over steps\n",
        "task1_model_args.gradient_accumulation_steps = 2 # Fancy hyperparameter related to batch size and memory\n",
        "\n",
        "task1_model = ClassificationModel(\"roberta\",\n",
        "                                  'roberta-base',\n",
        "                                  args = task1_model_args,\n",
        "                                  num_labels=2,\n",
        "                                  use_cuda=cuda_available)\n",
        "# train model\n",
        "task1_model.train_model(df_balanced[['text', 'label']])\n",
        "# run predictions\n",
        "preds_task1, _ = task1_model.predict(tedf1.text.tolist())"
      ],
      "metadata": {
        "id": "fHk6VFmHO0bO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
        "print('precision: ' + str(precision_score(tedf1.label.values, preds_task1)))\n",
        "print( 'recall： ' + str(recall_score(tedf1.label.values, preds_task1)))\n",
        "print('f1_score: ' + str(f1_score(tedf1.label.values, preds_task1)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qvyn7xcGMxqZ",
        "outputId": "435d9b05-d815-4146-f54d-0a553386be8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "precision: 0.4154727793696275\n",
            "recall： 0.7286432160804021\n",
            "f1_score: 0.5291970802919709\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing stop words and punctuation"
      ],
      "metadata": {
        "id": "4rkspneFP8OO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "# Download stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Function to remove stopwords and punctuation\n",
        "def clean_text(text):\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = ''.join([char for char in text if char not in string.punctuation])\n",
        "    # Remove stopwords\n",
        "    text = ' '.join([word for word in text.split() if word.lower() not in stop_words])\n",
        "    return text\n",
        "\n",
        "# Apply the function to your text data\n",
        "trdf1['text'] = trdf1['text'].apply(clean_text)\n",
        "tedf1['text'] = tedf1['text'].apply(clean_text)\n"
      ],
      "metadata": {
        "id": "_ZgzS4VxRdUi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19d25d56-0a65-4f4d-b261-a0731ac6626c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data_out = trdf1\n",
        "\n",
        "torch.manual_seed(2)\n",
        "\n",
        "# Best Hyperparameters\n",
        "task1_model_args.num_train_epochs = 5 # Number of epochs\n",
        "task1_model_args.learning_rate = 3.76e-05 # Learning rate\n",
        "task1_model_args.train_batch_size = 16 # Batch size\n",
        "task1_model_args.weight_decay = 0.0300861 # Regularization Strength\n",
        "task1_model_args.warmup_steps = 222 # lr starts at 0 - goes to 0.0001 over steps\n",
        "task1_model_args.gradient_accumulation_steps = 2 # Fancy hyperparameter related to batch size and memory\n",
        "\n",
        "# Initialize the model with the cleaned and preprocessed dataset\n",
        "task1_model_data_aug = ClassificationModel(\n",
        "    \"roberta\",\n",
        "    'roberta-base',\n",
        "    args=task1_model_args,\n",
        "    num_labels=2,\n",
        "    use_cuda=cuda_available\n",
        ")\n",
        "\n",
        "# Train the model with the preprocessed data\n",
        "task1_model_data_aug.train_model(data_out[['text', 'label']])\n",
        "\n",
        "preds_task1, _ = task1_model_data_aug.predict(tedf1['text'].tolist())"
      ],
      "metadata": {
        "id": "WeLxu5wwO4hL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
        "print('precision: ' + str(precision_score(tedf1.label.values, preds_task1)))\n",
        "print( 'recall： ' + str(recall_score(tedf1.label.values, preds_task1)))\n",
        "print('f1_score: ' + str(f1_score(tedf1.label.values, preds_task1)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLPmmpr1NcRo",
        "outputId": "f2635edc-b7c7-4cae-ab3e-3aab151b2a1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "precision: 0.5833333333333334\n",
            "recall： 0.4221105527638191\n",
            "f1_score: 0.489795918367347\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adding categorical data"
      ],
      "metadata": {
        "id": "REKXezSlXY1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate the categorical data with the text data\n",
        "trdf1['text'] = trdf1['community'] + ' ' + trdf1['text']\n",
        "tedf1['text'] = tedf1['community'] + ' ' + tedf1['text']\n",
        "\n",
        "torch.manual_seed(2)\n",
        "\n",
        "# Best Hyperparameters\n",
        "task1_model_args.num_train_epochs = 5 # Number of epochs\n",
        "task1_model_args.learning_rate = 3.76e-05 # Learning rate\n",
        "task1_model_args.train_batch_size = 16 # Batch size\n",
        "task1_model_args.weight_decay = 0.0300861 # Regularization Strength\n",
        "task1_model_args.warmup_steps = 222 # lr starts at 0 - goes to 0.0001 over steps\n",
        "task1_model_args.gradient_accumulation_steps = 2 # Fancy hyperparameter related to batch size and memory\n",
        "\n",
        "# Initialize the model with the cleaned and preprocessed dataset\n",
        "task1_model_data_aug = ClassificationModel(\n",
        "    \"roberta\",\n",
        "    'roberta-base',\n",
        "    args=task1_model_args,\n",
        "    num_labels=2,\n",
        "    use_cuda=cuda_available\n",
        ")\n",
        "\n",
        "\n",
        "# Continue with your model training and prediction as before\n",
        "task1_model_data_aug.train_model(trdf1[['text', 'label']])\n",
        "\n",
        "# Make predictions\n",
        "preds_task1, _ = task1_model_data_aug.predict(tedf1['text'].tolist())"
      ],
      "metadata": {
        "id": "JqLmSp3bRn9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
        "print('precision: ' + str(precision_score(tedf1.label.values, preds_task1)))\n",
        "print( 'recall： ' + str(recall_score(tedf1.label.values, preds_task1)))\n",
        "print('f1_score: ' + str(f1_score(tedf1.label.values, preds_task1)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoOpJT2RO3Px",
        "outputId": "6440fee2-01c5-42f7-b415-d78cee779c37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "precision: 0.5930232558139535\n",
            "recall： 0.5125628140703518\n",
            "f1_score: 0.5498652291105123\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K1Pz0xtYL9k_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}